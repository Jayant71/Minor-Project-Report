\chapter{Results and Discussion}
\vspace{-1.5cm}
\hspace{-1cm}\rule{19cm}{0.4pt} 

\section{Presentation of Results}
This chapter presents the results obtained from the development and functional testing of the AI-powered Book Reading Attention Monitoring system. The outcomes are detailed for each core module—gaze estimation, book detection—and for the integrated attention monitoring functionality. The results primarily focus on the system's ability to perform its intended tasks and provide the designed visual feedback.

\subsection{Gaze Estimation Module Performance}
The gaze estimation module, which utilizes the L2CS model \cite{L2CSNet2022}, is responsible for detecting the user's face and estimating the pitch and yaw angles of their eye gaze from the webcam feed.
\begin{itemize}
    \item \textbf{Functional Outcome:} The system successfully initializes the L2CS model and processes incoming video frames in real-time. Upon detecting a face, the module accurately computes and outputs the pitch and yaw values. Visual feedback, typically rendered as a vector or lines originating from the detected face (as facilitated by the \texttt{l2cs.render} utility or custom drawing functions), indicates the estimated gaze direction on the displayed video feed. The system demonstrated capability in tracking gaze across a reasonable range of head movements and orientations typical during a reading task.
    
    \item \textbf{Qualitative Observations:} [User to insert qualitative observations here. For example: "The gaze vector was observed to generally align with the user's perceived direction of sight under good lighting conditions. Performance was noted to be slightly affected by rapid head movements or partial face occlusions (e.g., hand near face)."]
    
    \item \textbf{Quantitative Performance (if tested):} [User to insert any quantitative evaluation of gaze estimation if conducted. For example, if you tested it against a ground truth or specific target points: "While a formal quantitative accuracy assessment of the L2CS model within this specific project setup was not performed due to X, Y, Z reasons, the model's published performance metrics (e.g., mean angular error on datasets like Gaze360) suggest a strong baseline. Informal tests within the project showed [describe any specific observations or simple test outcomes]."]
    
    \item \textbf{Visual Evidence:}
    % Figure placeholder for gaze estimation
    \begin{figure}[h!]
        \centering
        % \includegraphics[width=0.7\textwidth]{path/to/your/gaze_estimation_screenshot.png} % Replace with your actual image
        \framebox[0.7\textwidth][c]{\raisebox{0pt}[4cm][3cm]{Placeholder: Screenshot of Gaze Estimation}}
        \caption{Example of the gaze estimation module output, showing the detected face and the visualized gaze vector.}
        \label{fig:gaze_estimation_output}
    \end{figure}
    Figure~\ref{fig:gaze_estimation_output} should illustrate a sample output from the gaze estimation module. 
\end{itemize}

\subsection{Book Detection Module (Custom YOLOv12s) Performance}
The custom-trained YOLOv12s model is tasked with detecting physical books within the camera's field of view and classifying them as either "open\_book" or "closed\_book."
\begin{itemize}
    \item \textbf{Functional Outcome:} The integrated YOLOv12s model successfully processes video frames to identify and draw bounding boxes around books. The system distinguishes between open and closed books based on the trained classes. This functionality was observed to work across a variety of book types and orientations included in the training and validation datasets.
    
    \item \textbf{Training and Validation Metrics:} The YOLOv12s model was trained on a custom dataset comprising [User: Number] images, split into training ([User: Number/Percentage]), validation ([User: Number/Percentage]), and testing ([User: Number/Percentage]) sets. Key performance metrics achieved on the validation/test set during and after training are presented below:
    
    % Table placeholder for YOLO metrics
    \begin{table}[h!]
        \centering
        \caption{Performance Metrics for Custom YOLOv12s Book Detection Model.}
        \label{tab:yolo_metrics}
        \begin{tabular}{lccc}
            \hline
            \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} \\ \hline
            open\_book     & [User: Value]      & [User: Value]   & [User: Value]    \\
            closed\_book   & [User: Value]      & [User: Value]   & [User: Value]    \\ \hline
            \textbf{Overall/Average} & [User: Value] & [User: Value] & [User: Value (e.g., mAP@0.5-0.95)] \\ \hline
        \end{tabular}
    \end{table}
    \textit{[User: Please replace the bracketed values with your actual metrics from YOLO training. You can add more metrics like F1-score if available. You might also include a confusion matrix as a figure if you have one.]}

    \item \textbf{Qualitative Observations:} [User to insert qualitative observations here. For example: "The model demonstrated good performance in identifying books with clear covers and distinct shapes. Challenges were observed with books having highly reflective covers under direct light, or when books were significantly occluded or presented at extreme angles not well-represented in the training set. The distinction between 'open\_book' and 'closed\_book' was generally accurate when the book's state was visually unambiguous."]
    
    \item \textbf{Visual Evidence:}
    % Figure placeholder for book detection
    \begin{figure}[h!]
        \centering
        % \includegraphics[width=0.8\textwidth]{path/to/your/book_detection_examples.png} % Replace with your actual image (can be a composite of a few examples)
        \framebox[0.8\textwidth][c]{\raisebox{0pt}[5cm][4cm]{Placeholder: Examples of Book Detection (Correct \& Incorrect if any)}}
        \caption{Sample outputs from the book detection module, showing bounding boxes and class labels for "open\_book" and "closed\_book".}
        \label{fig:book_detection_output}
    \end{figure}
    Figure~\ref{fig:book_detection_output} should showcase examples of the book detection model in action. \textit{[User: Please replace the placeholder with actual screenshots from your system illustrating book detection.]}
\end{itemize}

\subsection{Integrated Attention Monitoring System Functionality}
The core result of the project is the integrated system's ability to infer attention by correlating gaze direction with the presence and state of an open book.
\begin{itemize}
    \item \textbf{Functional Outcome:} The system, through the \texttt{AttentionMonitor} module, successfully integrates the outputs from the gaze estimator and its internal book detector. It calculates the 3D gaze vector and performs the ray-book intersection test on a frame-by-frame basis. Based on this, it assigns an attention status.
    
    \item \textbf{Scenario-Based Observations:}
    \begin{itemize}
        \item \textbf{Attentive State:} When a user was looking directly at a detected "open\_book," the system correctly reported the status as "Attentive (Looking at open book)." The visual overlay on the display reflected this, typically showing the gaze vector directed towards the book's bounding box.
        \item \textbf{Distracted State:} When a user's gaze (while an "open\_book" was detected) was directed away from the book (e.g., looking at the ceiling, to the side), the system reported a status such as "Distracted (Open book detected, gaze elsewhere)."
        \item \textbf{Book Not Open/Present:} If a book was detected but classified as "closed\_book," or if no book was detected, the system provided an appropriate status message (e.g., "Closed book detected," "No book detected"), and the attention state was consequently not "Attentive."
        \item \textbf{Face Not Detected:} If the user's face was not detected by the gaze estimation module, the system would indicate "No face detected," and no attention assessment would be made.
    \end{itemize}
    \textit{[User: Elaborate on these scenarios with your specific observations. Were there any ambiguous cases or common misclassifications by the attention logic?]}

    \item \textbf{User Interface and Feedback:} The system provides real-time visual feedback through the main application window managed by the \texttt{SessionManager} and \texttt{CameraManager}. This includes:
    \begin{itemize}
        \item The live webcam feed.
        \item Overlays for the detected face bounding box.
        \item Visualization of the estimated gaze direction.
        \item Bounding boxes and class labels for detected books.
        \item A textual display of the current attention status message (e.g., "Attentive," "Distracted," "No book detected").
        \item An FPS (Frames Per Second) counter indicating processing speed.
    \end{itemize}
    
    \item \textbf{Visual Evidence:}
    % Figure placeholder for attention states
    \begin{figure}[h!]
        \centering
        % \includegraphics[width=\textwidth]{path/to/your/attention_states_composite.png} % Replace with a composite image showing different states
        \framebox[\textwidth][c]{\raisebox{0pt}[6cm][4cm]{Placeholder: Composite Screenshot of Different Attention States (Attentive, Distracted)}}
        \caption{Examples of the integrated system output, illustrating "Attentive" and "Distracted" states with corresponding visual cues.}
        \label{fig:attention_states_output}
    \end{figure}
    Figure~\ref{fig:attention_states_output} should demonstrate the system's output under different attention scenarios. \textit{[User: Please replace the placeholder with actual screenshots showcasing these states.]}
\end{itemize}

\subsection{System Performance Aspects}
Preliminary observations regarding the system's runtime performance were made during functional testing.
\begin{itemize}
    \item \textbf{Processing Speed (FPS):} The system achieved an average processing speed of approximately [User: XX-YY] FPS when running on [User: Specify hardware, e.g., a machine with an NVIDIA RTX XXXX GPU / an Intel Core iX CPU without dedicated GPU acceleration]. The FPS varied based on factors such as the number of faces/objects in the scene and background processing.
    \item \textbf{Responsiveness:} The visual feedback on the display was generally [User: Describe, e.g., fluid, with noticeable lag only under heavy load, etc.].
\end{itemize}
\textit{[User: Provide your observed FPS range and a brief comment on responsiveness. If you measured CPU/GPU utilization, you could add a sentence about that too.]}

In summary, the implemented system demonstrates the functional capability to detect faces, estimate gaze, detect books and their states, and integrate this information to provide a real-time, per-frame assessment of a user's visual attention towards a physical book, along with corresponding visual feedback.

\section{Interpretation of Results}
The results presented in the previous section (Section 6.1) provide valuable insights into the functional capabilities and performance characteristics of the developed Book Reading Attention Monitoring system. This section interprets these findings, discussing their significance in relation to the project's objectives and the overall efficacy of the implemented solution.

\subsection{Interpreting Gaze Estimation Performance}
The functional outcomes of the gaze estimation module, which leverages the L2CS model, are foundational to the entire attention monitoring pipeline.
\begin{itemize}
    \item \textbf{Successful Face Detection and Gaze Vector Generation:} The consistent ability of the system to detect faces and render a corresponding gaze vector (as would be shown in Figure~\ref{fig:gaze_estimation_output}) confirms the successful integration and operation of the L2CS model within the project's framework. This establishes that the primary input signal for inferring visual attention—the user's gaze direction (pitch and yaw)—is being effectively captured.
    
    \item \textbf{Implications of Qualitative Observations:} [User: Based on your qualitative observations from Section 6.1, interpret them here. For instance: "The observation that gaze vector alignment was generally accurate under good lighting signifies that the chosen model performs reliably in ideal conditions. Conversely, noted performance dips with rapid head movements or partial occlusions highlight the inherent sensitivities of appearance-based gaze estimation methods and define operational boundaries for optimal system use."]
    
    \item \textbf{Contextualizing Quantitative Performance (if applicable):} [User: If you included quantitative L2CS performance data: "The [specific metric, e.g., low mean angular error if cited from literature or tested] for the L2CS model suggests a degree of precision in gaze angle estimation that is generally sufficient for determining if the user's gaze is broadly directed towards or away from a relatively large target like a book. While not providing pinpoint accuracy on specific words, this level of detail is adequate for the project's aim of assessing general attention to the book area."]
\end{itemize}
Overall, the results from the gaze estimation module indicate that the system can reliably obtain the necessary directional gaze data from users under typical webcam usage conditions, forming a viable basis for subsequent attention analysis.

\subsection{Interpreting Book Detection Module Performance}
The custom-trained YOLOv12s model's performance is critical for contextualizing the user's gaze by identifying the object of interest (the book) and its state.
\begin{itemize}
    \item \textbf{Effectiveness of Custom Training:} The quantitative metrics presented in Table~\ref{tab:yolo_metrics} (e.g., mAP, precision, recall for "open\_book" and "closed\_book") directly reflect the success of the custom data collection and model training effort. [User: Interpret your specific metrics here. For example: "A mAP@0.5 score of [Your Value] indicates a strong capability of the model to accurately localize and classify books. High precision for the 'open\_book' class (e.g., [Your Value]) means that when the model identifies an open book, it is very likely correct, which is crucial for minimizing false positives in attention assessment. A satisfactory recall (e.g., [Your Value]) ensures that most open books are indeed detected, allowing for consistent monitoring."]
    
    \item \textbf{Significance of Class Distinction:} The model's ability to distinguish between "open\_book" and "closed\_book" is a key result. This classification is vital because reading attention is primarily relevant when a book is open. The successful implementation of this distinction means the system can filter scenarios appropriately before applying the gaze intersection logic.
    
    \item \textbf{Impact of Dataset Characteristics (from qualitative observations):} [User: Based on your qualitative observations of the YOLO model: "The qualitative findings, such as [mention specific challenges like performance with reflective covers or occlusions], suggest that while the model performs well on instances similar to its training data, its generalization to entirely novel or challenging visual conditions is an area for further dataset enhancement. This interprets the boundaries of the current model's reliability."]
    
    \item \textbf{Visual Confirmation:} The examples shown in Figure~\ref{fig:book_detection_output} visually corroborate the quantitative metrics, providing confidence in the model's practical ability to identify books in typical reading setups.
\end{itemize}
The results from the book detection module suggest that the custom-trained YOLOv12s model provides a reliable mechanism for identifying the primary target of reading attention and its relevant state, which is essential for the attention inference stage.

\subsection{Interpreting Integrated Attention Monitoring Functionality}
The core contribution of the project lies in the integration of gaze and book detection data to infer attention. The observed functionality across different scenarios (as would be depicted in Figure~\ref{fig:attention_states_output}) allows for the following interpretations:
\begin{itemize}
    \item \textbf{Validation of the Gaze-Book Intersection Logic:} The system's ability to correctly classify states as "Attentive" when the user's gaze (as per the visualized vector) intersects with a detected open book, and "Distracted" when the gaze is directed elsewhere, validates the fundamental data analysis technique (ray-box intersection) employed. This indicates that the geometric approach to correlating gaze and object location is functionally sound for this application.
    
    \item \textbf{Successful Information Fusion:} The correct assignment of attention states demonstrates that the data streams from the gaze estimator (pitch, yaw, face location) and the book detector (book location, book state) are being effectively fused and processed by the \texttt{AttentionMonitor} module.
    
    \item \textbf{Handling of Edge Cases:} The system's appropriate responses to scenarios such as "No face detected" or "No book detected" (or "Closed book detected") are important. It shows that the decision-making logic includes necessary prerequisite checks and does not attempt to infer attention when key information is missing, thereby improving the reliability of the "Attentive" or "Distracted" labels when they are provided.
    
    \item \textbf{Effectiveness of Visual Feedback:} The real-time overlays on the user interface—showing detected faces, gaze direction, book boxes, and the textual attention status—serve as a direct interpretation of the system's internal state. This feedback loop, even if only visual in the current implementation, is the primary mechanism through which the system aims to make users aware of their attention patterns. The clarity and timeliness of this feedback are crucial for the system to be perceived as useful. [User: Comment on how intuitive or effective you found this feedback during your testing.]
\end{itemize}
The successful functioning of the integrated system in differentiating attention states based on visual cues signifies that the core technical objective of creating a per-frame attention assessment pipeline has been achieved.

\subsection{Interpreting System Performance Aspects}
The observed system performance metrics, such as FPS, provide an interpretation of its real-world usability.
\begin{itemize}
    \item \textbf{Real-Time Capability:} [User: Based on your FPS results: "An average processing speed of [XX-YY] FPS on [Your Hardware] suggests that the system can operate in near real-time, providing a relatively fluid experience for the user. If the FPS is high, this indicates efficient implementation and model choices. If it's on the lower side, this may imply that the computational load is significant and could be a bottleneck on less powerful hardware, potentially affecting the user experience due to lag between action and feedback."]
    
    \item \textbf{Resource Utilization (if observed):} [User: "Observations regarding CPU/GPU usage indicate [describe]. This has implications for the system's deployability on various user machines and its potential to run alongside other applications without significant performance degradation."]
\end{itemize}
The system's performance characteristics suggest its current feasibility for interactive use, while also highlighting areas where optimization might be beneficial for broader accessibility.

In essence, the presented results, when interpreted collectively, indicate that the developed system successfully implements the foundational components for AI-powered book reading attention monitoring. The gaze and book detection modules function as intended, and their outputs are effectively combined to produce a per-frame assessment of visual attention that is communicated to the user.


\section{Discussion of Key Findings}
The results presented and interpreted in the preceding sections offer several key insights into the development and functional capabilities of the AI-powered Book Reading Attention Monitoring system. This section synthesizes these findings to highlight their broader implications in the context of the project's objectives.

\begin{itemize}
    \item \textbf{Feasibility of Integrated Gaze and Book-Aware Attention Monitoring:}
    A primary finding is the demonstrated feasibility of integrating real-time gaze estimation (using L2CS) with custom object detection (YOLOv12s for books) to infer a user's visual attention towards physical reading material using a standard webcam. The system successfully implemented the core pipeline: capturing user's visual cues, processing them through respective AI models, and applying a geometric intersection logic to determine per-frame attention status. This confirms that such an approach, which moves beyond screen-based attention or specialized hardware, is viable for a common reading scenario. This aligns with the project's central aim of developing a technologically accessible attention monitoring tool.

    \item \textbf{Effectiveness of Core AI Components in the Application Context:}
    The performance of the individual AI modules is crucial. The L2CS model provided functional gaze direction estimates suitable for determining broad focus areas (i.e., towards or away from a book) \textit{[User: Here, refer to your specific findings on gaze estimator's reliability/accuracy]}. Similarly, the custom-trained YOLOv12s model demonstrated its capability to detect "open\_book" and "closed\_book" instances with [User: e.g., "a promising level of accuracy (mAP of XX)" or "functional accuracy for clear scenarios"], as indicated by [User: your metrics/observations]. The ability to differentiate between an open and closed book was a key finding, as it directly impacts the relevance of attention assessment. These findings underscore the successful adaptation and application of these sophisticated models to the specific tasks defined by the project.

    \item \textbf{Significance of Gaze-Book Intersection Logic:}
    The successful operationalization of the ray-book intersection algorithm as the primary mechanism for attention inference is a significant finding. The system's ability to differentiate between "Attentive" (gaze intersecting with an open book) and "Distracted" states in clear-cut scenarios (as would be shown in your Figure~\ref{fig:attention_states_output}) validates this geometric approach. It highlights that a relatively straightforward computational geometry technique can be effectively combined with deep learning outputs to infer contextual attention. This method provides a more robust indicator of attention to a specific object (the book) than gaze direction alone.

    \item \textbf{Nature and Utility of Inferred Visual Attention:}
    The project successfully demonstrates the inference of *visual* attention. The system's output (e.g., "Attentive," "Distracted") is a direct reflection of whether the user's estimated gaze direction aligns with the detected location of an open book. This finding is important as it provides a quantifiable, albeit indirect, proxy for cognitive attention during reading. While not a measure of comprehension, this visual attention status can serve as valuable feedback for users aiming to improve their focus, aligning with the objective of promoting better reading habits.

    \item \textbf{Viability of Real-Time Operation:}
    The system's observed processing speed (FPS) [User: "of approximately XX-YY FPS on (specified hardware)"] indicates its viability for real-time or near real-time application. This is a key finding for any interactive system designed to provide immediate feedback. It suggests that the chosen models and implementation strategies offer a reasonable balance between analytical depth and computational efficiency for the target user experience.
\end{itemize}
In essence, the key findings converge to demonstrate that the developed system effectively establishes a proof-of-concept for AI-powered monitoring of visual attention during physical book reading. The successful integration of its core components and the functional attention inference mechanism achieve the primary technical objectives of the project, providing a foundation upon which more advanced features and evaluations can be built.

\section{Limitations and Future Directions}
While the project successfully demonstrates a functional prototype for book reading attention monitoring, the results and the development process also highlight several limitations. These limitations, in turn, open up numerous avenues for future research and system enhancement.

\subsection{Limitations Based on Current Findings}
\begin{itemize}
    \item \textbf{Accuracy and Robustness in Diverse Conditions:} As indicated by the qualitative observations [User: and any quantitative shortcomings you reported for L2CS/YOLO], the performance of both the L2CS gaze estimator and the custom YOLOv12s book detector can be sensitive to suboptimal environmental conditions (e.g., poor lighting, glare, cluttered backgrounds) and specific user-related factors (e.g., eyewear, extreme head poses, partial occlusions). This limits the system's reliability across the full spectrum of real-world reading scenarios.
    
    \item \textbf{Granularity and Nuance of Attention Assessment:} The current system primarily provides a binary per-frame attention status ("Attentive" vs. "Distracted"). This is a simplification of the complex nature of attention. The system does not currently:
    \begin{itemize}
        \item Quantify different levels or depths of attention.
        \item Robustly differentiate between brief, purposeful glances away (e.g., for reflection) and genuine distraction without more sophisticated temporal analysis.
        \item Account for cognitive engagement if visual gaze remains on the book (i.e., "looking without seeing").
    \end{itemize}

    \item \textbf{Generalizability of Custom Book Detector:} The performance of the YOLOv12s model is inherently tied to the diversity and scope of its custom training dataset. While efforts were made to create a varied dataset, it may not cover all possible book types, cover designs, sizes, or reading contexts, potentially limiting its generalizability [User: refer to any specific examples from your testing where it struggled].
    
    \item \textbf{Implementation of Session-Level Analytics and Alerts:} As noted in the Methodology and Scope, while the per-frame attention status is determined, the advanced features envisioned in the initial requirements—such as calculating overall session attention percentages, generating detailed user-facing logs for trend analysis, or implementing alerts based on sustained periods of inattention—are foundational in the current implementation. The core data is available, but the higher-level aggregation and trigger mechanisms require further development.

    \item \textbf{User Experience and Feedback Mechanism:} The current visual feedback, while informative, is relatively basic. The impact of this feedback on actual user behavior and focus improvement has not been formally evaluated through user studies. The optimal mode and frequency of feedback remain an open question.
\end{itemize}

\subsection{Future Directions}
The limitations identified pave the way for several exciting future research and development directions:

\begin{itemize}
    \item \textbf{Enhancing Model Robustness and Accuracy:}
    \begin{itemize}
        \item \textbf{Dataset Expansion:} Significantly expanding the custom book detection dataset with more diverse examples (various book types, lighting, occlusions, backgrounds) and employing advanced data augmentation techniques.
        \item \textbf{Advanced Models:} Exploring newer or more robust gaze estimation models (potentially those discussed in \cite{Kothari_GazeReviewDL_2024} or transformer-based approaches like in \cite{Huang_GazeEstimationTransformer_2023}) and more advanced YOLO architectures (e.g., YOLOv8, YOLOv9 \cite{ultralytics_yolov8_2023, Wang_YOLOv9_2024}) or other state-of-the-art object detectors.
        \item \textbf{Fine-tuning Pre-trained Models:} Further fine-tuning the L2CS model on a dataset more specific to reading scenarios if such data could be collected.
    \end{itemize}

    \item \textbf{Sophisticating Attention Analysis and Metrics:}
    \begin{itemize}
        \item \textbf{Temporal Analysis:} Implement logic to analyze attention patterns over time (e.g., using sliding windows, Hidden Markov Models, or LSTMs) to identify sustained periods of attention/inattention, frequency of distractions, and calculate overall session attention scores.
        \item \textbf{Levels of Engagement:} Explore methods to infer different levels of engagement, potentially by incorporating analysis of head pose dynamics, blink rates, or even rudimentary facial expression analysis if feasible with current models.
        \item \textbf{Contextual Alerts:} Develop intelligent alert mechanisms that trigger only after a configurable duration of sustained inattention, making them less intrusive.
    \end{itemize}

    \item \textbf{Improving User Interface and Feedback:}
    \begin{itemize}
        \item \textbf{Personalized Dashboards:} Develop a user dashboard to visualize attention patterns, trends over time, and provide personalized insights and suggestions.
        \item \textbf{Configurable Feedback:} Allow users to customize the type (visual, auditory) and frequency of attention feedback.
        \item \textbf{Gamification:} Introduce gamification elements to encourage sustained focus.
    \end{itemize}

    \item \textbf{Conducting Formal User Studies:}
    Perform comprehensive user studies with diverse participants to:
    \begin{itemize}
        \item Quantitatively evaluate the system's accuracy in real-world reading scenarios against ground truth measures of attention (e.g., self-reports, task performance, secondary task responses).
        \item Assess the usability and user experience (UX) of the system.
        \item Measure the actual impact of the system on users' reading focus and habits over time.
    \end{itemize}

    \item \textbf{Expanding System Scope:}
    \begin{itemize}
        \item \textbf{Digital Reading Support:} Adapt the system to monitor attention while reading on digital screens (monitors, tablets), which would require different methods for identifying the "reading material" area.
        \item \textbf{Integration with Learning Platforms:} Explore possibilities of integrating the attention data (with user consent) into e-learning platforms for adaptive learning experiences.
    \end{itemize}

    \item \textbf{Addressing Explainability and Trust:}
    Further explore methods to make the AI's decision-making process more transparent to the user, enhancing trust and understanding of the system's feedback.

    \item \textbf{Optimization for Broader Accessibility:}
    Investigate model quantization, pruning, or deployment on edge AI hardware to make the system more lightweight and accessible on a wider range of user devices without requiring powerful GPUs.
\end{itemize}
Addressing these future directions could significantly enhance the capabilities, robustness, and practical utility of the Book Reading Attention Monitoring system, moving it closer to a deployable tool for aiding reading concentration.