\chapter{Literature Review} 
\vspace{-1.5cm}
\hspace{-1cm}\rule{19cm}{0.4pt} 

\setstretch{1.5}
\normalsize

\section{Introduction}
This chapter reviews existing literature pertinent to the development of an AI-powered book reading attention monitoring system. The review is structured around three core technological pillars: gaze estimation techniques, object detection methodologies (with a focus on identifying books), and approaches to attention monitoring, particularly in contexts related to reading and learning. By examining prior work in these areas, this section aims to establish the foundation upon which the current project is built, highlight relevant advancements, and identify the existing landscape of research.

\section{Gaze Estimation Techniques}
Gaze estimation, the process of determining the direction or point of a user's gaze, is a critical component for understanding visual attention. Research in this area has evolved significantly, broadly categorized into model-based and appearance-based methods. Model-based methods typically involve geometric modeling of the eye, while appearance-based methods directly learn a mapping from eye or face images to gaze direction \cite{Cheng_GazeSurvey_2021}.

Recent advancements have heavily favored appearance-based approaches leveraging deep learning due to their robustness in unconstrained environments without requiring specialized hardware \cite{Kothari_GazeReviewDL_2024}. Convolutional Neural Networks (CNNs) have been extensively used to extract salient features from facial and eye regions for gaze regression. For instance, the L2CS-Net model, utilized in this project, employs a CNN architecture designed for fine-grained gaze estimation by regressing pitch and yaw angles independently, demonstrating strong performance on challenging datasets like Gaze360 \cite{L2CSNet2022}. Other recent research explores lightweight architectures for efficient gaze estimation \cite{Li_LightweightGaze_2023} and the application of transformers to this domain \cite{Huang_GazeEstimationTransformer_2023}, pushing the boundaries of accuracy and efficiency. The ability of such models to function with standard webcams makes them highly suitable for accessible attention monitoring applications.

\section{Object Detection for Contextual Awareness}
To understand if a user is attending to a specific item, such as a book, the system must first be able to detect and localize that item within the visual scene. Object detection has seen remarkable progress with the advent of deep learning, particularly with one-stage detectors like the You Only Look Once (YOLO) family. The original YOLO model introduced a paradigm shift by framing object detection as a single regression problem, enabling real-time performance \cite{Redmon_YOLO_2016}.

Subsequent iterations, including YOLOv5 \cite{Jocher_YOLOv5_2020}, YOLOv8 \cite{ultralytics_yolov8_2023}, and YOLOv9 \cite{Wang_YOLOv9_2024}, have progressively improved accuracy, speed, and versatility, making them state-of-the-art for various applications. These models are designed to identify multiple objects within an image, predict their bounding boxes, and classify them. Surveys on YOLO variants highlight their architectural evolution and wide adoption \cite{Terven_YOLOSurvey_2023, DiSalvo_YOLOSurvey_2021}. For this project, the ability of YOLO models to be custom-trained for specific object classes, such as "open\_book" and "closed\_book," is crucial. Research has also demonstrated the application of deep learning, including YOLO-based approaches, for specific tasks like book detection and document analysis \cite{Ahmad_BookDetection_2023}, underscoring the feasibility of this component. More recent developments like YOLO-NAS also aim to enhance object detection capabilities \cite{Deci_YOLONAS_2023}.

\section{Attention Monitoring and Reader Engagement}
Monitoring user attention, especially in educational and reading contexts, has garnered significant research interest. Attention is a complex cognitive state, and inferring it from observable behaviors is a challenging task. Computer vision offers non-intrusive methods to capture relevant cues.

Gaze direction is a primary indicator of overt visual attention \cite{article}. Studies have explored using gaze patterns to assess student engagement in classrooms \cite{Sharma_AttentionClassroom_2023} and online learning environments \cite{Li_StudentEngagementOnline_2023, Guo_StudentAttentionSurvey_2023}. The concept of a "Reading Companion" system that monitors reading behaviors, including attention, has been explored to provide assistance to readers \cite{Lai_ReadingCompanion_2017}. Furthermore, research indicates that gaze patterns can be correlated with cognitive load during reading \cite{Ghosh_CognitiveLoadReading_2022} and even reading comprehension \cite{Bao_GazeReadingComprehension_2023}.

Head pose and facial expressions are also valuable cues often integrated into multimodal attention monitoring systems \cite{Zhang_MultimodalAttention_2023, Kar_AttentiveLearner_2022}. A comprehensive survey by Albu et al. \cite{Albu_AttentionLearning_2023} discusses various deep learning techniques for engagement level estimation. The current project focuses on gaze and book presence, but the literature supports the value of these cues as strong indicators of attention towards reading material.

\section{Integrating Gaze with Object Context for Focused Attention}
While general gaze direction provides some information, understanding attention towards a *specific* object requires linking the gaze vector to the object's location. This is particularly relevant for determining if a reader is looking at their book. Research such as that by Zhu et al. \cite{Zhu_GazeObjectFocus_2023} explores methods for localizing an object prompted by gaze, effectively performing a gaze-object mapping. This project's core attention inference mechanism, which involves calculating the intersection of the user's gaze vector with the bounding box of a detected open book, aligns with this principle of contextualizing gaze. By confirming that the user's visual focus aligns with the location of the reading material, a more reliable inference of attention during the reading task can be made.

\section{Summary and Research Gap}
The literature demonstrates robust advancements in individual areas of gaze estimation, real-time object detection, and vision-based attention/engagement monitoring. Models like L2CS-Net offer accurate gaze tracking, while the YOLO family provides efficient object detection. Numerous studies have explored attention in learning and reading, often using gaze as a key metric.

While many systems track attention on screens or use specialized eye-tracking hardware, this project aims to synthesize these advancements to address the specific scenario of monitoring attention on physical books using a standard webcam. It combines a state-of-the-art gaze estimator with a custom-trained, efficient object detector to infer attention in a non-intrusive manner. The key contribution lies in the practical integration of these components for the nuanced task of discerning if a reader's focus is indeed on their physical reading material, providing a foundation for a tool that could aid in improving reading habits. The ethical considerations of such monitoring systems are also an emerging area of discussion relevant to the responsible development of these technologies \cite{Gupta_EthicalAIEd_2024}.

This review indicates that while the foundational technologies are well-established, their specific combination and application for real-time attention monitoring on physical books with commodity hardware represent a practical and relevant area of investigation.